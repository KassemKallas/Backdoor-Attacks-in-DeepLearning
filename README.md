# AI-Backdoor
The aim of this Github repository is to gather and summarise Backdoor in AI resources. In particular, we will list and present attacks and defenses on AI and Deep Learning systems together with code repositories if exist.
First, we will list the papers in chronological order from older to newer.
In addition, we will try to categorize the attacks and defenses based on the threat model, the amount of knowledge available to the attacker and the defender and the adversarial settings.

The main categories of attacks are: "clean label", "poisoned label", "blackbox", "whitebox", "full control", "partial control"

The main research contributions to improve the backdoor attack works on one of the following:
*reducing the backdoor signal (trigger) visibility to human visual system - stealthiness
*improve the backdoor robustness
*fine-tuning

The defences on the other hand work on one of three levels:
*Data
*Model
*Training Data


So, under each paper, we will try to classify the categories selected by the authors in order to make the work comparable with others.

Keywords: "Backdoor attacks", "Adversarial attacks", "Trojans", "Neural Trojans"

# Surveys on Backdoor attacks and defenses

# Available sources and toolboxes

# Backdoor Attacks on image and video models

# Backdoor Defenses for image and video models

# Benign use of Backdoor attacks
## DNN Watermarking

## Adversarial examples detection
